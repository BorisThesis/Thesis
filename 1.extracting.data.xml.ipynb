{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tarfile\n",
    "import xml.etree.ElementTree as ET\n",
    "from xml.dom.minidom import parse, parseString\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import re\n",
    "import csv\n",
    "from tqdm import tqdm\n",
    "import csv\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import xmltodict\n",
    "from collections.abc import MutableMapping\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pandas settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tar_extract_and_populate(location_input: str):\n",
    "    \"\"\"This function takes in a set of tar files, extracts them in a directory of choice and populates a list of files in the directory\"\"\"\n",
    "    \n",
    "    tar_data_files = [name for name in os.listdir(location_input)]\n",
    "    xml_files_list=[]\n",
    "    year = location_input[-4:]\n",
    "    \n",
    "    for tar_data_file in tar_data_files:\n",
    "        #open the tar.gz file as a tar archive\n",
    "        print(\"{}/{}\".format(location_input, tar_data_file))\n",
    "        tar_archive = tarfile.open(name=\"{}/{}\".format(location_input, tar_data_file), mode=\"r:gz\")\n",
    "\n",
    "        #extract the tar archive in a directory of choice\n",
    "        tar_archive.extractall(\"../new_data/extracted_data/\")\n",
    "\n",
    "        for file in tar_archive.getnames():\n",
    "            if file[-4:] == \".xml\":\n",
    "                xml_files_list.append(\"{}/{}\".format(location_input, file))\n",
    "\n",
    "        tar_archive.close()\n",
    "\n",
    "    return xml_files_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#directory = \"new_data/raw_data\"\n",
    "#xml_files_list = tar_extract_and_populate(directory)\n",
    "#with open(\"../3.new_data/files_list.pickle\", \"wb\") as f:\n",
    "#    pickle.dump(new_file_names, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../3.new_data/files_list.pickle\", \"rb\") as f:\n",
    "    files_list = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_dict(d, parent_key ='', sep ='_'):\n",
    "    items = []\n",
    "    for k, v in d.items():\n",
    "        new_key = parent_key + sep + k if parent_key else k\n",
    " \n",
    "        if isinstance(v, MutableMapping):\n",
    "            items.extend(flatten_dict(v, new_key, sep = sep).items())\n",
    "        else:\n",
    "            items.append((new_key, v))\n",
    "    return dict(items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = {}\n",
    "for i, file_path in tqdm(enumerate(files_list)):\n",
    "\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as fileptr:\n",
    "        xml_content = fileptr.read()\n",
    "\n",
    "    tender_dict = xmltodict.parse(xml_content, process_namespaces=False)\n",
    "    tender_flat_dict = flatten_dict(tender_dict)\n",
    "    data_dict[i] = tender_flat_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open(\"new_data/data_dict_440K_460K\", \"wb\") as f:\n",
    "#    pickle.dump(data_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data_dict_0_20k',\n",
       " 'data_dict_120K_160K',\n",
       " 'data_dict_160K_200K',\n",
       " 'data_dict_200K_240K',\n",
       " 'data_dict_20k_40k',\n",
       " 'data_dict_240K_280K',\n",
       " 'data_dict_280K_320K',\n",
       " 'data_dict_320K_360K',\n",
       " 'data_dict_360K_400K',\n",
       " 'data_dict_400K_440K',\n",
       " 'data_dict_40k_80K',\n",
       " 'data_dict_440K_480K',\n",
       " 'data_dict_460K_480K',\n",
       " 'data_dict_480K_520K',\n",
       " 'data_dict_520K_560K',\n",
       " 'data_dict_560K_600k',\n",
       " 'data_dict_600k_',\n",
       " 'data_dict_80K_120K']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#LOAD AND COMBINE ALL DATA\n",
    "directory = \"../3.new_data/\"\n",
    "data_dict_files = [name for name in os.listdir(directory) if \"data_dict_\" in name]\n",
    "data_dict_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = \"data_dict_0_20k\"\n",
    "with open(\"../3.new_data/{}\".format(file), \"rb\") as f:\n",
    "    data_dict = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "base = r'TED_EXPORT_FORM_SECTION_F\\d{2}_2014_OBJECT_CONTRACT_OBJECT_DESCR'\n",
    "\n",
    "short_descr_pattern = r'TED_EXPORT_FORM_SECTION_F\\d{2}_2014_OBJECT_CONTRACT_SHORT_DESCR_P(?!_.*)'\n",
    "object_descr_pattern_lots = base + r'(?!_.*)'\n",
    "\n",
    "duration_type_pattern = base + r'_DURATION_@TYPE'\n",
    "duration_text_pattern = base + r'_DURATION_#text'\n",
    "title_text_pattern = r'TED_EXPORT_FORM_SECTION_F03_2014_OBJECT_CONTRACT_TITLE_P'\n",
    "doc_id_pattern = r'0*([1-9][0-9]*|0[1-9][0-9]*)'\n",
    "date_start_pattern = base + r'_DATE_START'\n",
    "date_end_pattern = base + r'_DATE_END'\n",
    "\n",
    "ac_criterion_pattern = base + r'(_AC_AC_QUALITY|_AC_AC_COST)(_AC_CRITERION)?'\n",
    "ac_criterion_pattern_lots = r'(AC_AC_QUALITY|AC_AC_COST)'\n",
    "\n",
    "#OBJECT_CONTRACT_OBJECT_DESCR\n",
    "val_object_pattern = base + r'_VAL_OBJECT_#text'\n",
    "val_object_currency_pattern = base + r'_VAL_OBJECT_@CURRENCY'\n",
    "\n",
    "#OBJECT_CONTRACT\n",
    "base_val = r'TED_EXPORT_FORM_SECTION_F\\d{2}_2014_OBJECT_CONTRACT_VAL_'\n",
    "val_estimated_pattern = base_val + r'ESTIMATED_TOTAL_#text'\n",
    "val_estimated_currency_pattern = base_val + r'ESTIMATED_TOTAL_@CURRENCY'\n",
    "val_estimated_high_pattern = base_val + r'RANGE_TOTAL_HIGH'\n",
    "val_estimated_low_pattern = base_val + r'RANGE_TOTAL_LOW'\n",
    "val_total_pattern = base_val + r'TOTAL_#text'\n",
    "val_total_currency_pattern = base_val + r'TOTAL_@CURRENCY'\n",
    "\n",
    "\n",
    "#\n",
    "base_award = r'TED_EXPORT_FORM_SECTION_F\\d{2}_2014_AWARD_CONTRACT_AWARDED_CONTRACT_VALUES_VAL_'\n",
    "award_val_estimated_pattern = base_award + r'ESTIMATED_TOTAL_#text'\n",
    "award_val_estimated_currency_pattern = base_award + r'ESTIMATED_TOTAL_@CURRENCY'\n",
    "\n",
    "award_val_total_pattern = base_award + r'TOTAL_#text'\n",
    "award_val_total_currency_pattern = base_award + r'TOTAL_@CURRENCY'\n",
    "\n",
    "award_val_high_pattern = base_award + r'RANGE_TOTAL_HIGH'\n",
    "award_val_low_pattern = base_award + r'RANGE_TOTAL_LOW'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the row and short description out of the dict\n",
    "extracted_data = []\n",
    "\n",
    "#TED_EXPORT_FORM_SECTION_F03_2014_OBJECT_CONTRACT_SHORT_DESCR_P\n",
    "count = 0\n",
    "count_nested_dicts_in_list = 0\n",
    "\n",
    "#GO THROUGH ALL THE FILES\n",
    "for file in tqdm(list(data_dict_files)):\n",
    "    with open(\"../3.new_data/{}\".format(file), \"rb\") as f:\n",
    "        data_dict = pickle.load(f)\n",
    "        \n",
    "        #WITHIN EACH FILE, LOOP THROUGH EACH TENDER DICT\n",
    "        for i, file_number in enumerate(list(data_dict.keys())):\n",
    "            \n",
    "            #INITILIAZE THE DICT THAT IS TO BE FILLED WITH INFORMATION\n",
    "            tender = {\"ID_NOTICE_CAN\": \"\",\n",
    "                    \"LG_ORIG\": \"\",\n",
    "                    \"short description\": \"\",\n",
    "                    \"duration type\": \"\",\n",
    "                    \"duration\": \"\",\n",
    "                    \"title\": \"\",\n",
    "                    \"date start\": \"\",\n",
    "                    \"date end\": \"\",\n",
    "                    \"ac criterion\": \"\",\n",
    "\n",
    "                    \"value object\": \"\",\n",
    "                    \"value object currency\": \"\",\n",
    "                    \n",
    "                    \"obj_contract value estimated\": \"\",\n",
    "                    \"obj_contract value estimated currency\": \"\",\n",
    "                    \"obj_contract value estimated high\": \"\",\n",
    "                    \"obj_contract value estimated low\": \"\",\n",
    "                    \"obj_contract value total\": \"\",\n",
    "                    \"obj_contract value total currency\": \"\",\n",
    "                    \n",
    "                    \"award value estimated\": \"\",\n",
    "                    \"award value estimated currency\": \"\",\n",
    "                    \"award value total\": \"\",\n",
    "                    \"award value total currency\": \"\",\n",
    "                    \"award value high\": \"\",\n",
    "                    \"award value low\": \"\"}\n",
    "            \n",
    "            criteria_text = \"\"\n",
    "\n",
    "            #IF A TAG MATCHES SOME DEFINITION, STORE THE INFORMATION\n",
    "            for tag in data_dict[file_number].keys():\n",
    "                if \"@DOC_ID\" in tag:\n",
    "                    doc_id_list = str(data_dict[file_number][tag]).split(\"-\")\n",
    "                    doc_id_number = re.findall(doc_id_pattern, doc_id_list[0])\n",
    "                    doc_id = doc_id_list[1] + doc_id_number[0]\n",
    "                    tender[\"ID_NOTICE_CAN\"] = doc_id\n",
    "                \n",
    "                elif \"LG_ORIG\" in tag:\n",
    "                    if type(data_dict[file_number][tag]) == list:\n",
    "                        tender[\"LG_ORIG\"] = data_dict[file_number][tag][0]\n",
    "                    else:\n",
    "                        tender[\"LG_ORIG\"] = data_dict[file_number][tag]\n",
    "                \n",
    "                elif re.match(short_descr_pattern, tag):\n",
    "                    if type(data_dict[file_number][tag]) == list:\n",
    "                        if type(data_dict[file_number][tag][0]) == str:\n",
    "                            tender[\"short description\"] = data_dict[file_number][tag][0]\n",
    "                        else:\n",
    "                            count_nested_dicts_in_list += 1\n",
    "                    else:\n",
    "                        tender[\"short description\"] = data_dict[file_number][tag]  \n",
    "                elif re.match(title_text_pattern, tag):\n",
    "                    tender[\"title\"] = data_dict[file_number][tag]\n",
    "                elif re.match(ac_criterion_pattern, tag) and \"WEIGHTING\" not in tag:\n",
    "                    if type(data_dict[file_number][tag]) == list:\n",
    "                        for criteria_dict in data_dict[file_number][tag]:\n",
    "                            if criteria_dict[\"AC_CRITERION\"] not in criteria_text:\n",
    "                                criteria_text = criteria_text + criteria_dict[\"AC_CRITERION\"] + \" \"\n",
    "                            else:\n",
    "                                continue\n",
    "                    else:\n",
    "                        criteria_text = criteria_text + data_dict[file_number][tag] + \" \"\n",
    "                elif re.match(duration_type_pattern, tag):\n",
    "                    tender[\"duration type\"] = data_dict[file_number][tag]\n",
    "                elif re.match(duration_text_pattern, tag):\n",
    "                    tender[\"duration\"] = data_dict[file_number][tag]\n",
    "                elif re.match(date_start_pattern, tag):\n",
    "                    tender[\"date start\"] = data_dict[file_number][tag]\n",
    "                elif re.match(date_end_pattern, tag):\n",
    "                    tender[\"date end\"] = data_dict[file_number][tag]\n",
    "                \n",
    "                #object values\n",
    "                elif re.match(val_object_pattern, tag):\n",
    "                    tender[\"value object\"] = data_dict[file_number][tag]\n",
    "                elif re.match(val_object_currency_pattern, tag):\n",
    "                    tender[\"value object currency\"] = data_dict[file_number][tag]\n",
    "\n",
    "                #object_contract_val\n",
    "                elif re.match(val_estimated_pattern, tag):\n",
    "                    tender[\"obj_contract value estimated\"] = data_dict[file_number][tag]\n",
    "                elif re.match(val_estimated_currency_pattern, tag):\n",
    "                    tender[\"obj_contract value estimated currency\"] = data_dict[file_number][tag]\n",
    "                elif re.match(val_estimated_high_pattern, tag):\n",
    "                    tender[\"obj_contract value estimated high\"] = data_dict[file_number][tag]\n",
    "                elif re.match(val_estimated_low_pattern, tag):\n",
    "                    tender[\"obj_contract value estimated low\"] = data_dict[file_number][tag]\n",
    "                elif re.match(val_total_pattern, tag):\n",
    "                    tender[\"obj_contract value total\"] = data_dict[file_number][tag]\n",
    "                elif re.match(val_total_currency_pattern, tag):\n",
    "                    tender[\"obj_contract value total currency\"] = data_dict[file_number][tag]\n",
    "\n",
    "                #award values \n",
    "                elif re.match(award_val_estimated_pattern, tag):\n",
    "                    tender[\"award value estimated\"] = data_dict[file_number][tag]\n",
    "                elif re.match(award_val_estimated_currency_pattern, tag):\n",
    "                    tender[\"award value estimated currency\"] = data_dict[file_number][tag]\n",
    "                elif re.match(award_val_total_pattern, tag):\n",
    "                    tender[\"award value total\"] = data_dict[file_number][tag]\n",
    "                elif re.match(award_val_total_currency_pattern, tag):\n",
    "                    tender[\"award value total currency\"] = data_dict[file_number][tag]\n",
    "                elif re.match(award_val_high_pattern, tag):\n",
    "                    tender[\"award value high\"] = data_dict[file_number][tag]\n",
    "                elif re.match(award_val_low_pattern, tag):\n",
    "                    tender[\"award value low\"] = data_dict[file_number][tag]\n",
    "\n",
    "                #FOR EACH LOT IN THE TENDER, COPY MAIN DICT, AND FILL WITH INFORMATION PER LOT \n",
    "                elif re.match(object_descr_pattern_lots, tag) and type(data_dict[file_number][tag]) == list:\n",
    "                    count += 1\n",
    "                    for lot_number in range(0, len(data_dict[file_number][tag])):\n",
    "                        base_dict_checkpoint = copy.deepcopy(tender)\n",
    "                        flat_lot_notice = flatten_dict(data_dict[file_number][tag][lot_number])\n",
    "                        criteria_text_lot = \"\" #define the criteria at the lot level allow concatenation\n",
    "\n",
    "                        for variable in flat_lot_notice.keys():\n",
    "                            if variable == \"SHORT_DESCR_P\":\n",
    "                                if type(flat_lot_notice[\"SHORT_DESCR_P\"]) == list:\n",
    "                                    if type(flat_lot_notice[\"SHORT_DESCR_P\"][0]) == str:\n",
    "                                        base_dict_checkpoint[\"short description\"] = flat_lot_notice[\"SHORT_DESCR_P\"][0]\n",
    "                                    else:\n",
    "                                        count_nested_dicts_in_list +=1\n",
    "                                else:\n",
    "                                    base_dict_checkpoint[\"short description\"] = flat_lot_notice[\"SHORT_DESCR_P\"]\n",
    "                            elif variable == \"LOT_NO\":\n",
    "                                base_dict_checkpoint[\"ID_LOT\"] = flat_lot_notice[variable] #str(lot_number)\n",
    "                            elif \"DURATION_@TYPE\" in variable:\n",
    "                                base_dict_checkpoint[\"duration type\"] = flat_lot_notice[variable]\n",
    "                            elif \"DURATION_#text\" in variable:\n",
    "                                base_dict_checkpoint[\"duration\"] = flat_lot_notice[variable]\n",
    "                            elif \"DATE_START\" in variable:\n",
    "                                base_dict_checkpoint[\"date start\"]\n",
    "                            elif \"DATE_END\" in variable:\n",
    "                                base_dict_checkpoint[\"date end\"]\n",
    "                            elif \"TITLE_P\" in variable:\n",
    "                                base_dict_checkpoint[\"title\"] = flat_lot_notice[variable]\n",
    "                            elif re.match(ac_criterion_pattern_lots, variable)  and \"WEIGHTING\" not in variable:\n",
    "                                if type(flat_lot_notice[variable]) == list:\n",
    "                                    for criteria_dict in flat_lot_notice[variable]:\n",
    "                                        if criteria_dict[\"AC_CRITERION\"] not in criteria_text_lot:\n",
    "                                            criteria_text_lot = criteria_text_lot + criteria_dict[\"AC_CRITERION\"] + \" \"\n",
    "                                        else:\n",
    "                                            continue\n",
    "                                    base_dict_checkpoint[\"ac criterion\"] = criteria_text_lot\n",
    "\n",
    "                                elif flat_lot_notice[variable] != None:\n",
    "                                    criteria_text_lot = criteria_text_lot + flat_lot_notice[variable] + \" \"\n",
    "                                    base_dict_checkpoint[\"ac criterion\"] = criteria_text_lot\n",
    "                            else:\n",
    "                                continue\n",
    "\n",
    "                        #ADD NaN FOR MISSING VALUES\n",
    "                        for variable in base_dict_checkpoint.keys():\n",
    "                            if base_dict_checkpoint[variable] == None:\n",
    "                                base_dict_checkpoint[variable] = np.nan\n",
    "                            elif len(base_dict_checkpoint[variable]) < 1:\n",
    "                                base_dict_checkpoint[variable] = np.nan\n",
    "                            else:\n",
    "                                continue\n",
    "\n",
    "                        extracted_data.append(base_dict_checkpoint)\n",
    "\n",
    "                    tender = {}\n",
    "                else:\n",
    "                    continue\n",
    "\n",
    "            if len(tender) > 0:\n",
    "                for variable in tender.keys():\n",
    "                    if len(tender[variable]) < 1:\n",
    "                        tender[variable] = np.nan\n",
    "                    else:\n",
    "                        continue\n",
    "                \n",
    "                tender[\"ID_LOT\"] = -1\n",
    "                tender[\"ac criterion\"] = criteria_text\n",
    "                extracted_data.append(tender)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SAVE DICT\n",
    "#with open(\"../3.new_data/extracted_data_dict_id.pickle\", \"wb\") as f:\n",
    "#    pickle.dump(extracted_data, f)\n",
    "\n",
    "#MAKE DF FROM DICT\n",
    "df = pd.DataFrame.from_records(extracted_data)\n",
    "\n",
    "#SAVE DF\n",
    "#with open(\"../3.new_data/df_extracted_id_xml.pickle\", \"wb\") as f:\n",
    "#    pickle.dump(df, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
